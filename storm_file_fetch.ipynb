{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import gzip\n",
    "import os\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV COLUMN STRUCTURE\n",
    "https://www.ncei.noaa.gov/pub/data/swdi/stormevents/csvfiles/Storm-Data-Bulk-csv-Format.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we need to get all of the available files \n",
    "\n",
    "url = 'https://www.ncei.noaa.gov/pub/data/swdi/stormevents/csvfiles/'\n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.text, 'html.parser')\n",
    "\n",
    "# now we need to extract the file names from the parent directory\n",
    "links = []\n",
    "# this will grab all of the a href elements' links \n",
    "for link in [link['href'] for link in soup.find('table').find_all('a')]:\n",
    "    # then check that they're a details file (the ones we actually want)\n",
    "    if link[:29] == 'StormEvents_details-ftp_v1.0_':\n",
    "        links.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\nStarting to fetch files. This can take a couple minutes.\")\n",
    "out_df = None\n",
    "holding = 'data'\n",
    "# this needs a folder called 'data' on your local drive, so it will create one if it's not already there\n",
    "if not os.path.exists(holding):\n",
    "    os.makedirs(holding)\n",
    "total_files = len(links)\n",
    "\n",
    "# A lot happens here, but we start by iterating over all the files we've identified\n",
    "for idx,file_path in enumerate(links):\n",
    "    local_file_path = holding + '/' + file_path\n",
    "    # we build the target url with the file_path we extracted earlier\n",
    "    target = url + file_path\n",
    "    # we go get the file in question\n",
    "    urllib.request.urlretrieve(target, local_file_path)\n",
    "    # it's a gzip'd file, so we need to unzip it. \n",
    "    with gzip.open(local_file_path, 'rb') as f:\n",
    "        # create a df from the csv we pulled down \n",
    "        if out_df is None:\n",
    "            out_df = pd.read_csv(f)\n",
    "        else:\n",
    "            out_df = pd.concat([out_df,pd.read_csv(f)])\n",
    "    # be kind, don't pummel your free data sources with requests \n",
    "    sleep(1)\n",
    "    if idx % 5 == 0:\n",
    "        print(f\"{idx}/{total_files} complete\")\n",
    "out_df = out_df.reset_index()\n",
    "print('\\n','='*(100),'\\nFetching complete! Reducing master file\\n','='*(100),'\\n')\n",
    "\n",
    "out_df[out_df['EVENT_TYPE'].isin(['Tornado','TORNADOES'])][\n",
    "    [\n",
    "    'BEGIN_YEARMONTH',      # YYYYMM of event start\n",
    "    'BEGIN_DAY',            # DD of event start\n",
    "    'END_YEARMONTH',        # YYYYMM of event end\n",
    "    'END_DAY',              # DD of event end\n",
    "    'EPISODE_ID',           # NWS ID for storm\n",
    "    'EVENT_ID',             # NWS ID for event (different from episode, which is mostly for storm narratives)\n",
    "    'STATE',                # SPELLED OUT ALL CAPS state name\n",
    "    'STATE_FIPS',           # Federal Information Processing Standard ID of the state (good for joining to other data)\n",
    "    'EVENT_TYPE',           # may consider other options here for what the project is attempting to do \n",
    "    'CZ_TYPE',              # C: County/Parish, Z: NWS Public Forecast Zone, M: Marine\n",
    "    'CZ_FIPS',              # See above description of FIPS but for local region ID (like Adams County)\n",
    "    'CZ_NAME',              # Name of the CZ\n",
    "    'INJURIES_DIRECT',      # You can probably guess \n",
    "    'INJURIES_INDIRECT',    # Difference is annoying undefined\n",
    "    'DEATHS_DIRECT',        # You can probably guess, but sadder\n",
    "    'DEATHS_INDIRECT',      # Same idea\n",
    "    'DAMAGE_PROPERTY',      # Estimate damage to property, suffix units are all jumbled\n",
    "    'TOR_F_SCALE',          # Enhanced Fujita Scale of the tornado \n",
    "    'TOR_LENGTH',           # Length of tornado segment on the ground (in miles, nearest tenth)\n",
    "    'TOR_WIDTH',            # Width of tornado on the ground (feet)\n",
    "    'BEGIN_LAT',            # Latitude of beginning of damage path\n",
    "    'BEGIN_LON',            # Longitude of beginning of damage path\n",
    "    'EPISODE_NARRATIVE',    # NWS' narrative of the storm that created the tornado\n",
    "    'EVENT_NARRATIVE'       # NWS' narrative of the tornado itself\n",
    "    ]\n",
    "].to_csv('StormEvents_details_WORKING.csv')\n",
    "print('\\n','='*(100),'\\nWorking file created! Creating Master copy.\\n','='*(100),'\\n')\n",
    "out_df.to_csv('StormEvents_details_MASTER.csv')\n",
    "print('\\n','='*(100),'\\nMaster copy created! Enjoy your data!\\n','='*(100),'\\n')\n",
    "out_df = None \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ErdosMay2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
