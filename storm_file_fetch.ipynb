{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import pandas as pd\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import gzip\n",
    "import os\n",
    "from time import sleep\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV COLUMN STRUCTURE\n",
    "https://www.ncei.noaa.gov/pub/data/swdi/stormevents/csvfiles/Storm-Data-Bulk-csv-Format.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we need to get all of the available files \n",
    "\n",
    "url = 'https://www.ncei.noaa.gov/pub/data/swdi/stormevents/csvfiles/'\n",
    "html = get(url)\n",
    "soup = BeautifulSoup(html.text, 'html.parser')\n",
    "\n",
    "# now we need to extract the file names from the parent directory\n",
    "links = []\n",
    "# this will grab all of the a href elements' links \n",
    "for link in [link['href'] for link in soup.find('table').find_all('a')]:\n",
    "    # then check that they're a details file (the ones we actually want)\n",
    "    if link[:29] == 'StormEvents_details-ftp_v1.0_':\n",
    "        links.append(link)\n",
    "\n",
    "ef_remap = {\n",
    "    'F0':   'EF0',\n",
    "    'F1':   'EF1',\n",
    "    'EF0':  'EF0',\n",
    "    'F2':   'EF2',\n",
    "    'EF1':  'EF1',\n",
    "    'F3':   'EF3',\n",
    "    'EF2':  'EF2',\n",
    "    'EFU':  'EFU',\n",
    "    'F4':   'EF4',\n",
    "    'EF3':  'EF3',\n",
    "    'EF4':  'EF4',\n",
    "    'F5':   'EF5',\n",
    "    'EF5':  'EF5'\n",
    "}\n",
    "\n",
    "dtype = {\n",
    "    'BEGIN_YEARMONTH': int,\n",
    "    'BEGIN_DAY': int,\n",
    "    'BEGIN_TIME': 'Int64',\n",
    "    'END_YEARMONTH': int,\n",
    "    'END_DAY': int,\n",
    "    'END_TIME': int,\n",
    "    'EPISODE_ID': 'Int64',\n",
    "    'EVENT_ID': 'Int64',\n",
    "    'STATE': str,\n",
    "    'STATE_FIPS': 'Int64',\n",
    "    'YEAR': 'Int64',\n",
    "    'MONTH_NAME': str,\n",
    "    'EVENT_TYPE': str,\n",
    "    'CZ_TYPE': str,\n",
    "    'CZ_FIPS': 'Int64',\n",
    "    'CZ_NAME': str,\n",
    "    'WFO': str,\n",
    "    'BEGIN_DATE_TIME': str,\n",
    "    'CZ_TIMEZONE': str,\n",
    "    'END_DATE_TIME': str,\n",
    "    'INJURIES_DIRECT': 'Int64',\n",
    "    'INJURIES_INDIRECT': 'Int64',\n",
    "    'DEATHS_DIRECT': 'Int64',\n",
    "    'DEATHS_INDIRECT': 'Int64',\n",
    "    'DAMAGE_PROPERTY': str,\n",
    "    'DAMAGE_CROPS': str,\n",
    "    'SOURCE': str,\n",
    "    'MAGNITUDE': 'Float64',\n",
    "    'MAGNITUDE_TYPE': str,\n",
    "    'FLOOD_CAUSE': str,\n",
    "    'TOR_F_SCALE': str,\n",
    "    'TOR_LENGTH': 'Float64',\n",
    "    'TOR_WIDTH': 'Float64',\n",
    "    'TOR_OTHER_WFO': str,\n",
    "    'TOR_OTHER_CZ_STATE': str,\n",
    "    'TOR_OTHER_CZ_FIPS': 'Int64',\n",
    "    'TOR_OTHER_CZ_NAME': str,\n",
    "    'BEGIN_RANGE': 'Float64',\n",
    "    'BEGIN_AZIMUTH': str,\n",
    "    'BEGIN_LOCATION': str,\n",
    "    'END_RANGE': 'Float64',\n",
    "    'END_AZIMUTH': str,\n",
    "    'END_LOCATION': str,\n",
    "    'BEGIN_LAT': 'Float64',\n",
    "    'BEGIN_LON': 'Float64',\n",
    "    'END_LAT': 'Float64',\n",
    "    'END_LON': 'Float64',\n",
    "    'EPISODE_NARRATIVE': str,\n",
    "    'EVENT_NARRATIVE': str,\n",
    "    'DATA_SOURCE': str\n",
    "}\n",
    "\n",
    "def desuffix(r_):\n",
    "    print(r_)\n",
    "    if not(isinstance(r_,str)):\n",
    "        return 0\n",
    "    r = r_.upper()\n",
    "    val = -1\n",
    "    if 'K' in r:\n",
    "        try:\n",
    "            val = float(r[:-1]) * 1000\n",
    "        except:\n",
    "            val = 1000\n",
    "    elif 'M' in r:\n",
    "        try:\n",
    "            val = float(r[:-1]) * 1000000\n",
    "        except:\n",
    "            val = 1000000\n",
    "    elif 'B' in r:\n",
    "        val = float(r[:-1]) * 1000000000\n",
    "    else:\n",
    "        try:\n",
    "            val = float(r)\n",
    "        except:\n",
    "            val = 0\n",
    "    return int(val)\n",
    "\n",
    "def shout(r_):\n",
    "    val = 'N/A'\n",
    "    try:\n",
    "        val = r_.upper()\n",
    "    except:\n",
    "        pass\n",
    "    return val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\nStarting to fetch files. This can take a couple minutes.\")\n",
    "out_df = None\n",
    "holding = 'data'\n",
    "# this needs a folder called 'data' on your local drive, so it will create one if it's not already there\n",
    "if not os.path.exists(holding):\n",
    "    os.makedirs(holding)\n",
    "total_files = len(links)\n",
    "\n",
    "# A lot happens here, but we start by iterating over all the files we've identified\n",
    "for idx,file_path in enumerate(links):\n",
    "    local_file_path = holding + '/' + file_path\n",
    "    # we build the target url with the file_path we extracted earlier\n",
    "    target = url + file_path\n",
    "    # we go get the file in question\n",
    "    urllib.request.urlretrieve(target, local_file_path)\n",
    "    # it's a gzip'd file, so we need to unzip it. \n",
    "    with gzip.open(local_file_path, 'rb') as f:\n",
    "        # create a df from the csv we pulled down \n",
    "        if out_df is None:\n",
    "            out_df = pd.read_csv(f,dtype=dtype)\n",
    "        else:\n",
    "            out_df = pd.concat([out_df,pd.read_csv(f,dtype=dtype)])\n",
    "    # be kind, don't pummel your free data sources with requests \n",
    "    sleep(1)\n",
    "    if idx % 5 == 0:\n",
    "        print(f\"{idx}/{total_files} complete\")\n",
    "#out_df = out_df.reset_index() # this crashes the kernel sometimes, probably a memory issue \n",
    "print('\\n','='*(100),'\\nFetching complete! Beginning Pre-Processing\\n','='*(100),'\\n')\n",
    "\n",
    "# Remap Fujita scale\n",
    "# Tornado rating isn't precise, more of an comparable approximation of damage dealt\n",
    "# so this conversion shouldn't affect much analysis\n",
    "out_df['TOR_F_SCALE_MAPPED'] = out_df['TOR_F_SCALE'].map(ef_remap)\n",
    "\n",
    "# Capitalize columns that contain text \n",
    "for shout_col in ['STATE','MONTH_NAME','BEGIN_AZIMUTH','BEGIN_LOCATION','END_AZIMUTH','END_LOCATION']:\n",
    "    out_df[shout_col] = out_df[shout_col].apply(shout)\n",
    "\n",
    "# Normalizing damage values\n",
    "out_df['DAMAGE_CROPS_DESUFFIX'] = out_df['DAMAGE_CROPS'].apply(desuffix)\n",
    "out_df['DAMAGE_PROPERTY_DESUFFIX'] = out_df['DAMAGE_PROPERTY'].apply(desuffix)\n",
    "\n",
    "print('\\n','='*(100),'\\nPre-Processing complete! Reducing master file\\n','='*(100),'\\n')\n",
    "out_df[out_df['EVENT_TYPE'].isin(['Tornado','TORNADOES'])][\n",
    "    [\n",
    "    'BEGIN_YEARMONTH',      # YYYYMM of event start\n",
    "    'BEGIN_DAY',            # DD of event start\n",
    "    'END_YEARMONTH',        # YYYYMM of event end\n",
    "    'END_DAY',              # DD of event end\n",
    "    'EPISODE_ID',           # NWS ID for storm\n",
    "    'EVENT_ID',             # NWS ID for event (different from episode, which is mostly for storm narratives)\n",
    "    'STATE',                # SPELLED OUT ALL CAPS state name\n",
    "    'STATE_FIPS',           # Federal Information Processing Standard ID of the state (good for joining to other data)\n",
    "    'EVENT_TYPE',           # may consider other options here for what the project is attempting to do \n",
    "    'CZ_TYPE',              # C: County/Parish, Z: NWS Public Forecast Zone, M: Marine\n",
    "    'CZ_FIPS',              # See above description of FIPS but for local region ID (like Adams County)\n",
    "    'CZ_NAME',              # Name of the CZ\n",
    "    'INJURIES_DIRECT',      # You can probably guess \n",
    "    'INJURIES_INDIRECT',    # Difference is annoying undefined\n",
    "    'DEATHS_DIRECT',        # You can probably guess, but sadder\n",
    "    'DEATHS_INDIRECT',      # Same idea\n",
    "    'DAMAGE_PROPERTY',      # Estimate damage to property, suffix units are all jumbled\n",
    "    'TOR_F_SCALE',          # (Enhanced) Fujita Scale of the tornado \n",
    "    'TOR_LENGTH',           # Length of tornado segment on the ground (in miles, nearest tenth)\n",
    "    'TOR_WIDTH',            # Width of tornado on the ground (feet)\n",
    "    'BEGIN_LAT',            # Latitude of beginning of damage path\n",
    "    'BEGIN_LON',            # Longitude of beginning of damage path\n",
    "    'EPISODE_NARRATIVE',    # NWS' narrative of the storm that created the tornado\n",
    "    'EVENT_NARRATIVE'       # NWS' narrative of the tornado itself\n",
    "    ]\n",
    "].to_csv('StormEvents_details_WORKING.csv')\n",
    "print('\\n','='*(100),'\\nWorking file created! Creating Master copy.\\n','='*(100),'\\n')\n",
    "out_df.to_csv('StormEvents_details_MASTER.csv')\n",
    "print('\\n','='*(100),'\\nMaster copy created! Enjoy your data!\\n','='*(100),'\\n')\n",
    "out_df = None \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ErdosMay2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
